{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.model_selection import KFold,cross_validate\n",
    "from xgboost import XGBRegressor as xc\n",
    "from hyperopt import hp,fmin,tpe,Trials,partial\n",
    "from hyperopt.early_stop import no_progress_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read trainset\n",
    "df=pd.read_csv('../../data/features_trainset_NoCoTaB1.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df['pretty_formula']\n",
    "print('data shape：',df.shape)\n",
    "df.drop_duplicates(subset=['composition'], inplace=True)\n",
    "print('data shape：',df.shape)\n",
    "df=df.dropna(axis=1)\n",
    "df=df.dropna(axis=0)\n",
    "col_list=df.columns\n",
    "col_name=col_list[2:len(col_list)]\n",
    "featur=df[col_name].values\n",
    "target=df['D_max'].values\n",
    "X=featur\n",
    "Y=target\n",
    "del df['composition']\n",
    "print('data shape：',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalization\n",
    "y = df['D_max']\n",
    "\n",
    "scaler=preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RF\n",
    "param_grid = {\n",
    "    'n_estimators':hp.quniform('n_estimators',50,550,5),\n",
    "    'max_features':hp.quniform('max_features',3,29,1),\n",
    "    'max_depth':hp.quniform('max_depth',8,55,1),\n",
    "    'min_samples_split':hp.quniform('min_samples_split',2,10,1),\n",
    "    'min_impurity_decrease':hp.quniform(\"min_impurity_decrease\",0,5,0.1)\n",
    "}\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    reg = RFR(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             max_features=int(params['max_features']),\n",
    "             min_samples_split=int(params['min_samples_split']),\n",
    "             min_impurity_decrease=params['min_impurity_decrease'],\n",
    "             random_state = 12138,\n",
    "             verbose = False,\n",
    "             n_jobs=10)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=10,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best =fmin(hyperopt_objective,\n",
    "                     space = param_grid,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = max_evals,\n",
    "                     verbose = True,\n",
    "                     trials = trials,\n",
    "                     early_stop_fn = early_stop_fn\n",
    "                     )\n",
    "\n",
    "    print('\\n','best params:',params_best,'\\n')\n",
    "    return params_best,trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = RFR(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             max_features=int(params['max_features']),\n",
    "             min_samples_split=int(params['min_samples_split']),\n",
    "             min_impurity_decrease=int(params['min_impurity_decrease']),\n",
    "             random_state =12138,\n",
    "             verbose = False,\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50%|███████████████████████                       | 251/500 [20:49<20:39,  4.98s/trial, best loss: 0.8654694760509385]\n",
    "\n",
    " best params: {'max_depth': 14.0, 'max_features': 10.0, 'min_impurity_decrease': 0.0, 'min_samples_split': 2.0, 'n_estimators': 520.0} \n",
    "\n",
    "0.7517131968491946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVM\n",
    "param_grid = {\n",
    "    'C':hp.quniform('C',1,50,1),\n",
    "    'gamma':hp.quniform('gamma',0.1,0.45,0.005),\n",
    "    'epsilon':hp.quniform('epsilon',0,0.2,0.002)\n",
    "}\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    reg = SVR(C=int(params['C']),\n",
    "             epsilon=params['epsilon'],\n",
    "             gamma=params['gamma'],\n",
    "             kernel='rbf',\n",
    "             verbose = False\n",
    "             )\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best =fmin(hyperopt_objective,\n",
    "                     space = param_grid,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = max_evals,\n",
    "                     verbose = True,\n",
    "                     trials = trials,\n",
    "                     early_stop_fn = early_stop_fn\n",
    "                     )\n",
    "    print('\\n','best params:',params_best,'\\n')\n",
    "    return params_best,trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = SVR(C=int(params['C']),\n",
    "             epsilon=params['epsilon'],\n",
    "             gamma=params['gamma'],\n",
    "             kernel='rbf',\n",
    "             verbose = False)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 76%|███████████████████████████████████▏          | 382/500 [14:08<04:22,  2.22s/trial, best loss: 0.8824507192245843]\n",
    "\n",
    " best params: {'C': 7.0, 'epsilon': 0.016, 'gamma': 0.335} \n",
    "\n",
    "0.7469736411877232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##XGBoost\n",
    "def hyperopt_objective(params):\n",
    "    reg = xc(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             reg_lambda=params['reg_lambda'],\n",
    "             learning_rate=params['learning_rate'],\n",
    "             subsample=params['subsample'],\n",
    "             colsample_bytree=params['colsample_bytree'],\n",
    "             colsample_bynode=params['colsample_bynode'],\n",
    "             gamma = params['gamma'],\n",
    "             min_child_weight=params['min_child_weight'],\n",
    "             objective='reg:squarederror',\n",
    "             random_state = 12138,\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "param_grid_simple = {'n_estimators': hp.quniform(\"n_estimators\",150,450,3)\n",
    "                     ,\"learning_rate\": hp.quniform(\"learning_rate\",0.05,0.3,0.002)\n",
    "                     ,\"colsample_bytree\":hp.quniform(\"colsample_bytree\",0.3,1,0.1)\n",
    "                     ,\"colsample_bynode\":hp.quniform(\"colsample_bynode\",0.1,1,0.1)\n",
    "                     ,\"gamma\":hp.quniform(\"gamma\",0,15,0.2)\n",
    "                     ,\"reg_lambda\":hp.quniform(\"reg_lambda\",0,25,0.5)\n",
    "                     ,\"min_child_weight\":hp.quniform(\"min_child_weight\",0,50,0.5)\n",
    "                     ,\"max_depth\":hp.quniform(\"max_depth\",5,45,1)\n",
    "                     ,\"subsample\":hp.quniform(\"subsample\",0.5,1,0.1)\n",
    "                    }\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best = fmin(hyperopt_objective\n",
    "                       , space = param_grid_simple\n",
    "                       , algo = tpe.suggest\n",
    "                       , max_evals = max_evals\n",
    "                       , verbose=True\n",
    "                       , trials = trials\n",
    "                       , early_stop_fn = early_stop_fn\n",
    "                      )\n",
    "    print(\"\\n\",\"\\n\",\"best params: \", params_best,\n",
    "          \"\\n\")\n",
    "    return params_best, trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = xc(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             reg_lambda=params['reg_lambda'],\n",
    "             learning_rate=params['learning_rate'],\n",
    "             subsample=params['subsample'],\n",
    "             colsample_bytree=params['colsample_bytree'],\n",
    "             colsample_bynode=params['colsample_bynode'],\n",
    "             gamma = params['gamma'],\n",
    "             min_child_weight=params['min_child_weight'],\n",
    "             objective='reg:squarederror',\n",
    "             random_state = 12138,\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 64%|█████████████████████████████▌                | 322/500 [59:33<32:55, 11.10s/trial, best loss: 0.8014115979125579]\n",
    "\n",
    " \n",
    " best params:  {'colsample_bynode': 0.4, 'colsample_bytree': 0.5, 'gamma': 0.0, 'learning_rate': 0.094, 'max_depth': 15.0, 'min_child_weight': 0.0, 'n_estimators': 393.0, 'reg_lambda': 20.5, 'subsample': 1.0} \n",
    "\n",
    "0.7864601637150512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LightGBM\n",
    "param_grid = {\n",
    "    'n_estimators':hp.quniform('n_estimators',100,800,5),\n",
    "    'num_leaves':hp.quniform('num_leaves',10,400,5),\n",
    "    'learning_rate':hp.quniform('learning_rate',0.1,0.5,0.02),\n",
    "    'min_child_samples':hp.quniform('min_child_samples',1,40,1),\n",
    "    'reg_alpha':hp.quniform(\"reg_alpha\",0,10,0.5),\n",
    "    'reg_lambda':hp.quniform(\"reg_lambda\",0,100,2),\n",
    "    'subsample':hp.quniform(\"subsample\",0.5,1,0.1)\n",
    "}\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    reg = lgb.LGBMRegressor(\n",
    "             n_estimators=int(params['n_estimators']),\n",
    "             num_leaves=int(params['num_leaves']),\n",
    "             min_child_samples=int(params['min_child_samples']),\n",
    "             learning_rate=params['learning_rate'],\n",
    "             reg_alpha=params['reg_alpha'],\n",
    "             reg_lambda=params['reg_lambda'], \n",
    "             subsample = params['subsample'],\n",
    "             random_state = 12138,\n",
    "             verbose = int(False),\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best =fmin(hyperopt_objective,\n",
    "                     space = param_grid,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = max_evals,\n",
    "                     verbose = True,\n",
    "                     trials = trials,\n",
    "                     early_stop_fn = early_stop_fn\n",
    "                     )\n",
    "    print('\\n','best params:',params_best,'\\n')\n",
    "    return params_best,trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = lgb.LGBMRegressor(\n",
    "             n_estimators=int(params['n_estimators']),\n",
    "             num_leaves=int(params['num_leaves']),\n",
    "             min_child_samples=int(params['min_child_samples']),\n",
    "             learning_rate=params['learning_rate'],\n",
    "             reg_alpha=params['reg_alpha'],\n",
    "             reg_lambda=params['reg_lambda'], \n",
    "             subsample = params['subsample'],\n",
    "             random_state = 12138,\n",
    "             verbose = int(False),\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 36%|████████████████▋                             | 181/500 [15:08<26:40,  5.02s/trial, best loss: 0.8214159831752198]\n",
    "\n",
    " best params: {'learning_rate': 0.2, 'min_child_samples': 13.0, 'n_estimators': 375.0, 'num_leaves': 150.0, 'reg_alpha': 1.5, 'reg_lambda': 64.0, 'subsample': 1.0} \n",
    "\n",
    "0.7749460709516753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ternary\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_scaled,y,train_size=0.8,random_state=12138)\n",
    "\n",
    "reg_svm = SVR(C=7,epsilon=0.016,gamma=0.335)\n",
    "reg_rf = RFR(max_depth=14,max_features=10,min_impurity_decrease=0,min_samples_split=2,n_estimators=520,random_state=12138)\n",
    "reg_xgb = xc(colsample_bynode=0.4,colsample_bytree=0.5,gamma=0,learning_rate=0.094,max_depth=15,\n",
    "         min_child_weight=0,n_estimators=393,reg_lambda=20.5,subsample=1,random_state=12138)\n",
    "reg_gbm = lgb.LGBMRegressor(learning_rate = 0.2,min_child_samples = 13,n_estimators = 375, \n",
    "         num_leaves =150,reg_alpha = 1.5,reg_lambda = 64,subsample = 1,random_state=12138)\n",
    "estimators = [('svm',reg_svm), ('rf',reg_rf), ('xgb',reg_xgb), ('lightgbm',reg_gbm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VC_hard =VotingRegressor(estimators).fit(X_train, Y_train)\n",
    "print('Test',VC_hard.score(X_test,Y_test))\n",
    "print('Train',VC_hard.score(X_train,Y_train))\n",
    "\n",
    "reg_rf.fit(X_train, Y_train)\n",
    "reg_xgb.fit(X_train, Y_train)\n",
    "reg_svm.fit(X_train, Y_train)\n",
    "reg_gbm.fit(X_train, Y_train)\n",
    "\n",
    "print('SVM',reg_svm.score(X_train, Y_train),reg_svm.score(X_test,Y_test))\n",
    "print('RF',reg_rf.score(X_train, Y_train),reg_rf.score(X_test,Y_test))\n",
    "print('XGB',reg_xgb.score(X_train, Y_train),reg_xgb.score(X_test,Y_test))\n",
    "print('LightGBM',reg_gbm.score(X_train, Y_train),reg_gbm.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_space = {\n",
    "                'weight1': hp.quniform(\"weight1\",0,1,0.01),\n",
    "                'weight2': hp.quniform(\"weight2\",0,1,0.01),\n",
    "                'weight3': hp.quniform(\"weight3\",0,1,0.01),\n",
    "                'weight4': hp.quniform(\"weight4\",0,1,0.01)\n",
    "}\n",
    "\n",
    "def hyperopt_objective_weight(params):\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    weight4 = params['weight4']\n",
    "    weights = [weight1, weight2, weight3,weight4]\n",
    "    \n",
    "    reg = VotingRegressor(estimators=estimators, n_jobs=-1,weights=weights)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=8,error_score='raise')\n",
    "    return -np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "    \n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(50)\n",
    "    params_best = fmin(hyperopt_objective_weight\n",
    "                       , space = params_space\n",
    "                       , algo = tpe.suggest\n",
    "                       , max_evals = max_evals\n",
    "                       , verbose=True\n",
    "                       , trials = trials\n",
    "                       , early_stop_fn = early_stop_fn\n",
    "                      )\n",
    "\n",
    "    print(\"\\n\",\"\\n\",\"best params: \", params_best,\n",
    "          \"\\n\")\n",
    "    return params_best, trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = VotingRegressor(estimators=estimators,n_jobs=-1,\n",
    "                          weights=[params[0]['weight1'],params[0]['weight2'],params[0]['weight3'],params[0]['weight4']])\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best = param_hyperopt(300)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best = param_hyperopt(300)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best = param_hyperopt(300)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78%|███████████████████████████████████          | 234/300 [49:47<14:02, 12.77s/trial, best loss: -0.7969370492417509]\n",
    "\n",
    " \n",
    " best params:  {'weight1': 0.4, 'weight2': 0.0, 'weight3': 0.99, 'weight4': 0.68} \n",
    "\n",
    "0.793676480147095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.4\n",
    "weight2 = 0\n",
    "weight3 = 0.99\n",
    "weight4 = 0.68\n",
    "\n",
    "weights = [weight1, weight2, weight3, weight4]\n",
    "\n",
    "weight_sum = weight1 + weight2 + weight3 + weight4\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_scaled,y,train_size=0.8,random_state=12138)\n",
    "Voting_soft_weight = VotingRegressor(estimators=estimators, n_jobs=-1,\n",
    "                                      weights=weights).fit(X_train, Y_train)\n",
    "print(Voting_soft_weight.score(X_train, Y_train))\n",
    "print(Voting_soft_weight.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=pd.read_csv('../../data/features_CoTaB.csv')\n",
    "compositions=df_feature['composition']\n",
    "del df_feature['Unnamed: 0']\n",
    "del df_feature['composition']\n",
    "del df_feature['pretty_formula'] \n",
    "print('data shape：',df_feature.shape)\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X = df_feature[col_name]\n",
    "pred_X_transformed=scaler.transform(pred_X)\n",
    "pred_X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.4\n",
    "weight2 = 0\n",
    "weight3 = 0.99\n",
    "weight4 = 0.68\n",
    "weights = [weight1, weight2, weight3, weight4]\n",
    "Voting_soft_weight = VotingRegressor(estimators=estimators, n_jobs=-1,\n",
    "                                      weights=weights).fit(X_scaled, y)\n",
    "\n",
    "weight_sum = weight1 + weight2 + weight3\n",
    "xxx = Voting_soft_weight.predict(pred_X_transformed)\n",
    "df_pred_CoTaB=pd.DataFrame({'pretty_formula':compositions,'pred_D_max':xxx})\n",
    "df_pred_CoTaB['pred_D_max'].describe()\n",
    "df_pred_CoTaB.to_csv('CoTaB1_fusion_D_max.csv')\n",
    "comp_1=[]\n",
    "comp_2=[]\n",
    "comp_3=[]\n",
    "points=[]\n",
    "for i in range(0,101,1):\n",
    "    for j in range(0,101-i,1):\n",
    "        k=100-i-j\n",
    "        comp_1.append(i)\n",
    "        comp_2.append(j)\n",
    "        comp_3.append(k)\n",
    "        points.append((i,j,k))\n",
    "\n",
    "D_max=df_pred_CoTaB['pred_D_max'].values\n",
    "data=dict()\n",
    "for x in range(0,len(D_max)):\n",
    "    data[points[x]]=D_max[x]\n",
    "    \n",
    "scale=100\n",
    "figure,tax = ternary.figure(scale=scale)\n",
    "figure.set_size_inches((15,12))\n",
    "figure.set_facecolor('w')\n",
    "tax.boundary(linewidth=1.5)\n",
    "tax.gridlines(color='blue',multiple=10,linewidth=0.5,alpha=0.7)\n",
    "tax.ticks(axis='lbr',linewidth=1,multiple=20,fontsize=25,offset=0.02)\n",
    "\n",
    "tax.clear_matplotlib_ticks()\n",
    "tax.get_axes().axis('off')\n",
    "\n",
    "tax.heatmap(data, scale=scale,style=\"h\", vmin=min(D_max), vmax=max(D_max), cmap='coolwarm',use_rgba=False, colorbar=True)\n",
    "tax.left_axis_label(r\"$\\leftarrow$ B\", fontsize=25, offset=0.105)\n",
    "tax.right_axis_label(r\"$\\leftarrow$ Ta\", fontsize=25, offset=0.105)\n",
    "tax.bottom_axis_label(\"Co \"+r\"$\\rightarrow$\", fontsize=25, offset=0.02)\n",
    "tax.savefig('CoTaB1_fusion_D_max.jpg',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
