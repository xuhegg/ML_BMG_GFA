{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.model_selection import KFold,cross_validate\n",
    "from xgboost import XGBRegressor as xc\n",
    "from hyperopt import hp,fmin,tpe,Trials,partial\n",
    "from hyperopt.early_stop import no_progress_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read trainset\n",
    "df=pd.read_csv('../../data/features_trainset_NoZrAlCu3.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df['pretty_formula']\n",
    "print('data shape：',df.shape)\n",
    "df.drop_duplicates(subset=['composition'], inplace=True)\n",
    "print('data shape：',df.shape)\n",
    "df=df.dropna(axis=1)\n",
    "df=df.dropna(axis=0)\n",
    "col_list=df.columns\n",
    "col_name=col_list[2:len(col_list)]\n",
    "featur=df[col_name].values\n",
    "target=df['D_max'].values\n",
    "X=featur\n",
    "Y=target\n",
    "del df['composition']\n",
    "print('data shape：',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddf=pd.read_csv('../../data/features_ZrAlCu.csv')\n",
    "del dddf['Unnamed: 0']\n",
    "del dddf['pretty_formula']\n",
    "#del ddf['composition']\n",
    "print(dddf.shape)\n",
    "for i in range(0,len(dddf)):\n",
    "    if dddf['composition'][i]=='Zr20 Al20 Cu60':\n",
    "        print(i)\n",
    "    if dddf['composition'][i]=='Zr20 Al25 Cu55':\n",
    "        print(i)\n",
    "    if dddf['composition'][i]=='Zr40 Al20 Cu40':\n",
    "        print(i)\n",
    "    if dddf['composition'][i]=='Zr45 Al20 Cu35':\n",
    "        print(i)\n",
    "    if dddf['composition'][i]=='Zr50 Al5 Cu45':\n",
    "        print(i)\n",
    "    if dddf['composition'][i]=='Zr55 Al5 Cu40':\n",
    "        print(i)\n",
    "dddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = dddf[1855:1856][col_name]\n",
    "df_2 = df_2.append(dddf[1850:1851][col_name],ignore_index=True)\n",
    "df_2 = df_2.append(dddf[3280:3281][col_name],ignore_index=True)\n",
    "df_2 = df_2.append(dddf[3575:3576][col_name],ignore_index=True)\n",
    "df_2 = df_2.append(dddf[3830:3831][col_name],ignore_index=True)\n",
    "df_2 = df_2.append(dddf[4075:4076][col_name],ignore_index=True)\n",
    "df_2['D_max']=[0,0,0.2,0.2,2,1.5]\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_2,ignore_index=True)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalization\n",
    "y = df['D_max']\n",
    "X = df[col_name]\n",
    "\n",
    "scaler=preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RF\n",
    "param_grid = {\n",
    "    'n_estimators':hp.quniform('n_estimators',50,550,5),\n",
    "    'max_features':hp.quniform('max_features',3,29,1),\n",
    "    'max_depth':hp.quniform('max_depth',8,55,1),\n",
    "    'min_samples_split':hp.quniform('min_samples_split',2,10,1),\n",
    "    'min_impurity_decrease':hp.quniform(\"min_impurity_decrease\",0,5,0.1)\n",
    "}\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    reg = RFR(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             max_features=int(params['max_features']),\n",
    "             min_samples_split=int(params['min_samples_split']),\n",
    "             min_impurity_decrease=params['min_impurity_decrease'],\n",
    "             random_state = 12138,\n",
    "             verbose = False,\n",
    "             n_jobs=10)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=10,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best =fmin(hyperopt_objective,\n",
    "                     space = param_grid,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = max_evals,\n",
    "                     verbose = True,\n",
    "                     trials = trials,\n",
    "                     early_stop_fn = early_stop_fn\n",
    "                     )\n",
    "    print('\\n','best params:',params_best,'\\n')\n",
    "    return params_best,trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = RFR(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             max_features=int(params['max_features']),\n",
    "             min_samples_split=int(params['min_samples_split']),\n",
    "             min_impurity_decrease=int(params['min_impurity_decrease']),\n",
    "             random_state =12138,\n",
    "             verbose = False,\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 46%|█████████████████████▎                        | 232/500 [13:10<15:13,  3.41s/trial, best loss: 0.6560922633165779]\n",
    "\n",
    " best params: {'max_depth': 23.0, 'max_features': 8.0, 'min_impurity_decrease': 0.0, 'min_samples_split': 3.0, 'n_estimators': 390.0} \n",
    "\n",
    "0.7336058999919889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVM\n",
    "param_grid = {\n",
    "    'C':hp.quniform('C',1,50,1),\n",
    "    'gamma':hp.quniform('gamma',0.1,0.45,0.005),\n",
    "    'epsilon':hp.quniform('epsilon',0,0.2,0.002)\n",
    "}\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    reg = SVR(C=int(params['C']),\n",
    "             epsilon=params['epsilon'],\n",
    "             gamma=params['gamma'],\n",
    "             kernel='rbf',\n",
    "             verbose = False\n",
    "             )\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best =fmin(hyperopt_objective,\n",
    "                     space = param_grid,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = max_evals,\n",
    "                     verbose = True,\n",
    "                     trials = trials,\n",
    "                     early_stop_fn = early_stop_fn\n",
    "                     )\n",
    "    print('\\n','best params:',params_best,'\\n')\n",
    "    return params_best,trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = SVR(C=int(params['C']),\n",
    "             epsilon=params['epsilon'],\n",
    "             gamma=params['gamma'],\n",
    "             kernel='rbf',\n",
    "             verbose = False)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80%|█████████████████████████████████████▌         | 400/500 [33:16<08:19,  4.99s/trial, best loss: 0.629500149685196]\n",
    "\n",
    " best params: {'C': 7.0, 'epsilon': 0.04, 'gamma': 0.375} \n",
    "\n",
    "0.73568261064387970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##XGBoost\n",
    "def hyperopt_objective(params):\n",
    "    reg = xc(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             reg_lambda=params['reg_lambda'],\n",
    "             learning_rate=params['learning_rate'],\n",
    "             subsample=params['subsample'],\n",
    "             colsample_bytree=params['colsample_bytree'],\n",
    "             colsample_bynode=params['colsample_bynode'],\n",
    "             gamma = params['gamma'],\n",
    "             min_child_weight=params['min_child_weight'],\n",
    "             objective='reg:squarederror',\n",
    "             random_state = 12138,\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "param_grid_simple = {'n_estimators': hp.quniform(\"n_estimators\",150,450,3)\n",
    "                     ,\"learning_rate\": hp.quniform(\"learning_rate\",0.05,0.3,0.002)\n",
    "                     ,\"colsample_bytree\":hp.quniform(\"colsample_bytree\",0.3,1,0.1)\n",
    "                     ,\"colsample_bynode\":hp.quniform(\"colsample_bynode\",0.1,1,0.1)\n",
    "                     ,\"gamma\":hp.quniform(\"gamma\",0,15,0.2)\n",
    "                     ,\"reg_lambda\":hp.quniform(\"reg_lambda\",0,25,0.5)\n",
    "                     ,\"min_child_weight\":hp.quniform(\"min_child_weight\",0,50,0.5)\n",
    "                     ,\"max_depth\":hp.quniform(\"max_depth\",5,45,1)\n",
    "                     ,\"subsample\":hp.quniform(\"subsample\",0.5,1,0.1)\n",
    "                    }\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best = fmin(hyperopt_objective\n",
    "                       , space = param_grid_simple\n",
    "                       , algo = tpe.suggest\n",
    "                       , max_evals = max_evals\n",
    "                       , verbose=True\n",
    "                       , trials = trials\n",
    "                       , early_stop_fn = early_stop_fn\n",
    "                      )\n",
    "    print(\"\\n\",\"\\n\",\"best params: \", params_best,\n",
    "          \"\\n\")\n",
    "    return params_best, trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = xc(n_estimators=int(params['n_estimators']),\n",
    "             max_depth=int(params['max_depth']),\n",
    "             reg_lambda=params['reg_lambda'],\n",
    "             learning_rate=params['learning_rate'],\n",
    "             subsample=params['subsample'],\n",
    "             colsample_bytree=params['colsample_bytree'],\n",
    "             colsample_bynode=params['colsample_bynode'],\n",
    "             gamma = params['gamma'],\n",
    "             min_child_weight=params['min_child_weight'],\n",
    "             objective='reg:squarederror',\n",
    "             random_state = 12138,\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24%|██████████▋                                 | 122/500 [30:32<1:34:37, 15.02s/trial, best loss: 0.6087342281968906]\n",
    "\n",
    " \n",
    " best params:  {'colsample_bynode': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.0, 'learning_rate': 0.08, 'max_depth': 17.0, 'min_child_weight': 6.0, 'n_estimators': 255.0, 'reg_lambda': 24.5, 'subsample': 0.7000000000000001} \n",
    "\n",
    "0.7605680694435468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LightGBM\n",
    "param_grid = {\n",
    "    'n_estimators':hp.quniform('n_estimators',100,800,5),\n",
    "    'num_leaves':hp.quniform('num_leaves',10,400,5),\n",
    "    'learning_rate':hp.quniform('learning_rate',0.1,0.5,0.02),\n",
    "    'min_child_samples':hp.quniform('min_child_samples',1,40,1),\n",
    "    'reg_alpha':hp.quniform(\"reg_alpha\",0,10,0.5),\n",
    "    'reg_lambda':hp.quniform(\"reg_lambda\",0,100,2),\n",
    "    'subsample':hp.quniform(\"subsample\",0.5,1,0.1)\n",
    "}\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    reg = lgb.LGBMRegressor(\n",
    "             n_estimators=int(params['n_estimators']),\n",
    "             num_leaves=int(params['num_leaves']),\n",
    "             min_child_samples=int(params['min_child_samples']),\n",
    "             learning_rate=params['learning_rate'],\n",
    "             reg_alpha=params['reg_alpha'],\n",
    "             reg_lambda=params['reg_lambda'], \n",
    "             subsample = params['subsample'],\n",
    "             random_state = 12138,\n",
    "             verbose = int(False),\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='neg_root_mean_squared_error',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "\n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(100)\n",
    "    params_best =fmin(hyperopt_objective,\n",
    "                     space = param_grid,\n",
    "                     algo = tpe.suggest,\n",
    "                     max_evals = max_evals,\n",
    "                     verbose = True,\n",
    "                     trials = trials,\n",
    "                     early_stop_fn = early_stop_fn\n",
    "                     )\n",
    "    print('\\n','best params:',params_best,'\\n')\n",
    "    return params_best,trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = lgb.LGBMRegressor(\n",
    "             n_estimators=int(params['n_estimators']),\n",
    "             num_leaves=int(params['num_leaves']),\n",
    "             min_child_samples=int(params['min_child_samples']),\n",
    "             learning_rate=params['learning_rate'],\n",
    "             reg_alpha=params['reg_alpha'],\n",
    "             reg_lambda=params['reg_lambda'], \n",
    "             subsample = params['subsample'],\n",
    "             random_state = 12138,\n",
    "             verbose = int(False),\n",
    "             n_jobs=-1)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_best,trials = param_hyperopt(500) \n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33%|███████████████▎                              | 167/500 [08:45<17:27,  3.15s/trial, best loss: 0.6110591870390076]\n",
    "\n",
    " best params: {'learning_rate': 0.14, 'min_child_samples': 23.0, 'n_estimators': 550.0, 'num_leaves': 155.0, 'reg_alpha': 0.0, 'reg_lambda': 90.0, 'subsample': 0.7000000000000001} \n",
    "\n",
    "0.7598072884186303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fusion\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ternary\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_scaled,y,train_size=0.8,random_state=12138)\n",
    "\n",
    "reg_svm = SVR(C=7,epsilon=0.04,gamma=0.375)\n",
    "reg_rf =RFR(max_depth=23,max_features=8,min_impurity_decrease=0,min_samples_split=3,n_estimators=390,random_state=12138)\n",
    "reg_xgb = xc(colsample_bynode=1,colsample_bytree=0.8,gamma=0,learning_rate=0.08,max_depth=17,\n",
    "         min_child_weight=6,n_estimators=255,reg_lambda=24.5,subsample=0.7,random_state=12138)\n",
    "reg_gbm =lgb.LGBMRegressor(learning_rate = 0.14,min_child_samples = 23,n_estimators =550, \n",
    "        num_leaves = 155,reg_alpha =0,reg_lambda =90,subsample = 0.7,random_state=12138)\n",
    "estimators = [('svm',reg_svm), ('rf',reg_rf), ('xgb',reg_xgb), ('lightgbm',reg_gbm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VC_hard =VotingRegressor(estimators).fit(X_train, Y_train)\n",
    "print('Test',VC_hard.score(X_test,Y_test))\n",
    "print('Train',VC_hard.score(X_train,Y_train))\n",
    "\n",
    "reg_rf.fit(X_train, Y_train)\n",
    "reg_xgb.fit(X_train, Y_train)\n",
    "reg_svm.fit(X_train, Y_train)\n",
    "reg_gbm.fit(X_train, Y_train)\n",
    "\n",
    "print('SVM',reg_svm.score(X_train, Y_train),reg_svm.score(X_test,Y_test))\n",
    "print('RF',reg_rf.score(X_train, Y_train),reg_rf.score(X_test,Y_test))\n",
    "print('XGB',reg_xgb.score(X_train, Y_train),reg_xgb.score(X_test,Y_test))\n",
    "print('LightGBM',reg_gbm.score(X_train, Y_train),reg_gbm.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_space = {\n",
    "                'weight1': hp.quniform(\"weight1\",0,1,0.01),\n",
    "                'weight2': hp.quniform(\"weight2\",0,1,0.01),\n",
    "                'weight3': hp.quniform(\"weight3\",0,1,0.01),\n",
    "                'weight4': hp.quniform(\"weight4\",0,1,0.01)\n",
    "}\n",
    "\n",
    "def hyperopt_objective_weight(params):\n",
    "    weight1 = params['weight1']\n",
    "    weight2 = params['weight2']\n",
    "    weight3 = params['weight3']\n",
    "    weight4 = params['weight4']\n",
    "    weights = [weight1, weight2, weight3,weight4]\n",
    "    \n",
    "    reg = VotingRegressor(estimators=estimators, n_jobs=-1,weights=weights)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=12138)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=8,error_score='raise')\n",
    "    return -np.mean(abs(validation_loss['test_score']))\n",
    "\n",
    "def param_hyperopt(max_evals=100):\n",
    "    \n",
    "    trials = Trials()\n",
    "    early_stop_fn = no_progress_loss(50)\n",
    "    params_best = fmin(hyperopt_objective_weight\n",
    "                       , space = params_space\n",
    "                       , algo = tpe.suggest\n",
    "                       , max_evals = max_evals\n",
    "                       , verbose=True\n",
    "                       , trials = trials\n",
    "                       , early_stop_fn = early_stop_fn\n",
    "                      )\n",
    "\n",
    "    print(\"\\n\",\"\\n\",\"best params: \", params_best,\n",
    "          \"\\n\")\n",
    "    return params_best, trials\n",
    "\n",
    "def hyperopt_validation(params):\n",
    "    reg = VotingRegressor(estimators=estimators,n_jobs=-1,\n",
    "                          weights=[params[0]['weight1'],params[0]['weight2'],params[0]['weight3'],params[0]['weight4']])\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=2)\n",
    "    validation_loss = cross_validate(reg,X_scaled,y,scoring='r2',cv=cv,\n",
    "                                    verbose=False,n_jobs=-1,error_score='raise')\n",
    "    return np.mean(abs(validation_loss['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best = param_hyperopt(300)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best = param_hyperopt(300)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best = param_hyperopt(300)\n",
    "hyperopt_validation(params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70%|█████████████████████████████▉             | 209/300 [1:14:26<32:24, 21.37s/trial, best loss: -0.7908373474093446]\n",
    "\n",
    " \n",
    " best params:  {'weight1': 0.75, 'weight2': 0.0, 'weight3': 0.7000000000000001, 'weight4': 0.6900000000000001} \n",
    "\n",
    "0.7757512998599477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.75\n",
    "weight2 = 0\n",
    "weight3 = 0.7\n",
    "weight4 = 0.69\n",
    "\n",
    "weights = [weight1, weight2, weight3, weight4]\n",
    "\n",
    "weight_sum = weight1 + weight2 + weight3 + weight4\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_scaled,y,train_size=0.8,random_state=12138)\n",
    "Voting_soft_weight = VotingRegressor(estimators=estimators, n_jobs=-1,\n",
    "                                      weights=weights).fit(X_train, Y_train)\n",
    "print(Voting_soft_weight.score(X_train, Y_train))\n",
    "print(Voting_soft_weight.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=pd.read_csv('../../data/features_ZrAlCu.csv')\n",
    "compositions=df_feature['composition']\n",
    "del df_feature['Unnamed: 0']\n",
    "del df_feature['composition']\n",
    "del df_feature['pretty_formula'] \n",
    "print('data shape：',df_feature.shape)\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X = df_feature[col_name]\n",
    "pred_X_transformed=scaler.transform(pred_X)\n",
    "pred_X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = 0.75\n",
    "weight2 = 0\n",
    "weight3 = 0.7\n",
    "weight4 = 0.69\n",
    "weights = [weight1, weight2, weight3, weight4]\n",
    "Voting_soft_weight = VotingRegressor(estimators=estimators, n_jobs=-1,\n",
    "                                      weights=weights).fit(X_scaled, y)\n",
    "\n",
    "weight_sum = weight1 + weight2 + weight3\n",
    "xxx = Voting_soft_weight.predict(pred_X_transformed)\n",
    "df_pred_ZrAlCu=pd.DataFrame({'pretty_formula':compositions,'pred_D_max':xxx})\n",
    "df_pred_ZrAlCu['pred_D_max'].describe()\n",
    "df_pred_ZrAlCu.to_csv('ZrAlCu_iteration3_D_max.csv')\n",
    "comp_1=[]\n",
    "comp_2=[]\n",
    "comp_3=[]\n",
    "points=[]\n",
    "for i in range(0,101,1):\n",
    "    for j in range(0,101-i,1):\n",
    "        k=100-i-j\n",
    "        comp_1.append(i)\n",
    "        comp_2.append(j)\n",
    "        comp_3.append(k)\n",
    "        points.append((i,j,k))\n",
    "\n",
    "D_max=df_pred_ZrAlCu['pred_D_max'].values\n",
    "data=dict()\n",
    "for x in range(0,len(D_max)):\n",
    "    data[points[x]]=D_max[x]\n",
    "    \n",
    "scale=100\n",
    "figure,tax = ternary.figure(scale=scale)\n",
    "figure.set_size_inches((10,8))\n",
    "figure.set_facecolor('w')\n",
    "tax.boundary(linewidth=1.5)\n",
    "tax.gridlines(color='blue',multiple=10,linewidth=0.5,alpha=0.7)\n",
    "tax.ticks(axis='lbr',linewidth=1,multiple=20,fontsize=20,offset=0.02)\n",
    "\n",
    "tax.clear_matplotlib_ticks()\n",
    "tax.get_axes().axis('off')\n",
    "\n",
    "tax.heatmap(data, scale=scale,style=\"h\", vmin=min(D_max), vmax=max(D_max), cmap='coolwarm',use_rgba=False, colorbar=True)\n",
    "tax.left_axis_label(r\"$\\leftarrow$ Cu\", fontsize=30, offset=0.12)\n",
    "tax.right_axis_label(r\"$\\leftarrow$ Al\", fontsize=30, offset=0.12)\n",
    "tax.bottom_axis_label(\"Zr \"+r\"$\\rightarrow$\", fontsize=30, offset=0.04)\n",
    "tax.savefig('ZrAlCu_iteration3_D_max.jpg',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = np.sort(df_pred_ZrAlCu['pred_D_max'])\n",
    "P = int(0.95 * len(sorted_data))\n",
    "threshold = sorted_data[P]\n",
    "\n",
    "print(f\"90%阈值为：{threshold}\")\n",
    "\n",
    "D_max_criteria=threshold\n",
    "points_D_max = []\n",
    "points_value=[]\n",
    "comp_1=[]\n",
    "comp_2=[]\n",
    "comp_3=[]\n",
    "points=[]\n",
    "for i in range(0,101,1):\n",
    "    for j in range(0,101-i,1):\n",
    "        k=100-i-j\n",
    "        comp_1.append(i)\n",
    "        comp_2.append(j)\n",
    "        comp_3.append(k)\n",
    "        points.append((i,j,k))\n",
    "A=comp_1\n",
    "B=comp_2\n",
    "C=comp_3\n",
    "\n",
    "scale=100\n",
    "fontsize=30\n",
    "figure,tax = ternary.figure(scale=scale)\n",
    "figure.set_size_inches((10,8))\n",
    "figure.set_facecolor('w')\n",
    "tax.boundary(linewidth=1.5)\n",
    "tax.gridlines(color='blue',multiple=10,linewidth=0.5,alpha=0.7)\n",
    "tax.ticks(axis='lbr',linewidth=1,multiple=20,fontsize=20,offset=0.02)\n",
    "tax.clear_matplotlib_ticks()\n",
    "tax.get_axes().axis('off')\n",
    "\n",
    "for i in range(0,len(df_pred_ZrAlCu)):\n",
    "    if df_pred_ZrAlCu['pred_D_max'][i]>D_max_criteria:\n",
    "        if A[i]>0 and B[i]>0 and C[i]>0:\n",
    "            points_D_max.append((A[i],B[i],C[i]))\n",
    "            points_value.append(df_pred_ZrAlCu['pred_D_max'][i])\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "if len(points_D_max)>0:\n",
    "    D_max=points_value\n",
    "    data=dict()\n",
    "    for x in range(0,len(D_max)):\n",
    "        data[points_D_max[x]]=D_max[x]\n",
    "    \n",
    "    tax.heatmap(data, scale=scale,style=\"h\", vmin=min(D_max), vmax=max(D_max), cmap='coolwarm',use_rgba=False, colorbar=True)\n",
    "    \n",
    "\n",
    "tax.left_axis_label(r\"$\\leftarrow$ \"+'Cu', fontsize=fontsize, offset=0.12)\n",
    "tax.right_axis_label(r\"$\\leftarrow$ \"+'Al', fontsize=fontsize, offset=0.12)\n",
    "tax.bottom_axis_label('Zr'+\" \"+r\"$\\rightarrow$\", fontsize=fontsize, offset=0.04)\n",
    "tax.savefig('ZrAlCu+2+2+2.tif',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
